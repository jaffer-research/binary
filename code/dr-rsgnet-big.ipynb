{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset as is with a WeightedRandomSampler to oversample classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0', '1', '2', '3', '4']\n",
      "Training set class counts: [20643  1934  4221   713   575]\n",
      "Total images: 35108, Training: 28086, Validation: 7022\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Define the transformation (in this case, just convert to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Path to your dataset folder (with subfolders for each class)\n",
    "data_dir = '/Users/jeff/code/dataset/dr/resized_train_cropped'\n",
    "\n",
    "# Load the dataset with ImageFolder\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "print(\"Classes:\", full_dataset.classes)\n",
    "\n",
    "# Calculate the sizes for the training and validation sets (80:20 split)\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# --- Oversampling code for the training set ---\n",
    "# Note: train_dataset is a Subset. Its 'indices' attribute holds the indices from the original dataset.\n",
    "# Extract the labels for each sample in the training subset.\n",
    "train_labels = [full_dataset.targets[i] for i in train_dataset.indices]\n",
    "\n",
    "# Compute the count for each class in the training set:\n",
    "class_counts = np.bincount(train_labels)\n",
    "print(\"Training set class counts:\", class_counts)\n",
    "\n",
    "# Calculate weights for each class: inverse frequency\n",
    "weights_per_class = 1.0 / class_counts\n",
    "\n",
    "# Assign a weight to each sample in the training subset based on its class label:\n",
    "samples_weights = [weights_per_class[label] for label in train_labels]\n",
    "samples_weights = torch.DoubleTensor(samples_weights)\n",
    "\n",
    "# Create the WeightedRandomSampler for the training DataLoader\n",
    "sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders using the sampler for the training set\n",
    "train_dl = DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=4)\n",
    "valid_dl = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Total images: {dataset_size}, Training: {train_size}, Validation: {val_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building The RSG_Net CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RSGNet224(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(RSGNet224, self).__init__()\n",
    "        \n",
    "        # --- Block 1 ---\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        #self.dropout1 = nn.Dropout(p=0.5)\n",
    "        #self.dropout1 = nn.Dropout(p=0.3)\n",
    "\n",
    "        # --- Block 2 ---\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        #self.dropout2 = nn.Dropout(p=0.5)\n",
    "        #self.dropout2 = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128*56*56, 128)  # after two poolings (224->112->56)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout_fc = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        #x = self.dropout1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        #x = self.dropout2(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_fc(x)\n",
    "\n",
    "        x = self.fc2(x)  # raw logits for CrossEntropyLoss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using the class-based approach from the RSGNet224 snippet:\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = RSGNet224(num_classes=5)  # Instantiate your model class\n",
    "model = model.to(device)          # Move model parameters to the selected device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model With Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train accuracy: 0.2343, Val accuracy: 0.0249\n",
      "Epoch 2 - Train accuracy: 0.2703, Val accuracy: 0.0281\n",
      "Epoch 3 - Train accuracy: 0.3265, Val accuracy: 0.0413\n",
      "Epoch 4 - Train accuracy: 0.4404, Val accuracy: 0.0510\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m torch.manual_seed(\u001b[32m1\u001b[39m)\n\u001b[32m     59\u001b[39m num_epochs = \u001b[32m20\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m hist = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, num_epochs, train_dl, valid_dl)\u001b[39m\n\u001b[32m     25\u001b[39m optimizer.step()\n\u001b[32m     26\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * y_batch.size(\u001b[32m0\u001b[39m)\n\u001b[32m     29\u001b[39m preds = torch.argmax(outputs, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     30\u001b[39m running_corrects += (preds == y_batch).float().sum().cpu().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()  # for multi-class\n",
    "weights = torch.tensor([0.272, 2.877, 1.327, 8.052, 9.918], dtype=torch.float)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    loss_hist_train = [0] * num_epochs\n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            running_corrects += (preds == y_batch).float().sum().cpu().item()\n",
    "\n",
    "        loss_hist_train[epoch] = running_loss / len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] = running_corrects / len(train_dl.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss_val = 0.0\n",
    "        running_corrects_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                outputs = model(x_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                \n",
    "                running_loss_val += loss.item() * y_batch.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                running_corrects_val += (preds == y_batch).float().sum().cpu().item()\n",
    "\n",
    "        loss_hist_valid[epoch] = running_loss_val / len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] = running_corrects_val / len(valid_dl.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} - Train accuracy: {accuracy_hist_train[epoch]:.4f}, Val accuracy: {accuracy_hist_valid[epoch]:.4f}')\n",
    "\n",
    "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid\n",
    "\n",
    "torch.manual_seed(1)\n",
    "num_epochs = 20\n",
    "hist = train(model, num_epochs, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_arr = np.arange(len(hist[0])) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "ax.plot(x_arr, hist[3], '--<', label='Validation acc.')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Accuracy', size=15)\n",
    "\n",
    "#plt.savefig('figures/14_17.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-class metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_true, y_pred = evaluate_model(model, valid_dl, device)\n",
    "\n",
    "# Print per-class metrics (precision, recall, F1-score)\n",
    "print(classification_report(y_true, y_pred, target_names=[str(i) for i in range(5)]))\n",
    "\n",
    "# Optionally, compute the confusion matrix to calculate sensitivity and specificity manually\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute per-class sensitivity (Recall) and specificity\n",
    "num_classes = cm.shape[0]\n",
    "sensitivity = {}\n",
    "specificity = {}\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    TN = cm.sum() - (TP + FN + FP)\n",
    "    \n",
    "    sensitivity[i] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity[i] = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "print(\"Per-class Sensitivity (Recall):\", sensitivity)\n",
    "print(\"Per-class Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Annotate each cell with the numeric value\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment='center',\n",
    "                     color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assume 'cm' is your confusion matrix and you have 5 classes (0-4)\n",
    "class_names = [str(i) for i in range(5)]\n",
    "plot_confusion_matrix(cm, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-py3.13",
   "language": "python",
   "name": "cnn-py3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
